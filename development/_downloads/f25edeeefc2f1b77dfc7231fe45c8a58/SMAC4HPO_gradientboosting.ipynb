{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Optimizing GradientBoosting with SMAC4HPO\n\n\nAn example for the usage of SMAC within Python.\nWe optimize a GradientBoosting on an artificially created binary classification dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport numpy as np\n\nfrom ConfigSpace.hyperparameters import UniformFloatHyperparameter, UniformIntegerHyperparameter\n\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom smac.configspace import ConfigurationSpace\nfrom smac.facade.smac_hpo_facade import SMAC4HPO\nfrom smac.scenario.scenario import Scenario\n\n# load data and split it into training and test dataset\nX, y = make_hastie_10_2(random_state=0)\nX_train, X_test = X[:8400], X[8400:]\ny_train, y_test = y[:8400], y[8400:]\n\n# Gradient Boosting scored with cross validation\n\n\ndef xgboost_from_cfg(cfg):\n    clf = GradientBoostingClassifier(**cfg, random_state=0).fit(X_train, y_train)\n    scores = cross_val_score(clf, X_train, y_train)\n    return 1 - np.mean(scores)\n\n\nlogging.basicConfig(level=logging.INFO)\n\n# creating a Configuration Space with every parameter over which SMAC is going to optimize\ncs = ConfigurationSpace()\n\nmax_depth = UniformIntegerHyperparameter(\"max_depth\", 1, 10, default_value=3)\ncs.add_hyperparameter(max_depth)\n\nlearning_rate = UniformFloatHyperparameter(\"learning_rate\", 0.01, 1.0, default_value=1.0, log=True)\ncs.add_hyperparameter(learning_rate)\n\nmin_samples_split = UniformFloatHyperparameter(\"min_samples_split\", 0.01, 1.0, default_value=0.1, log=True)\nmax_features = UniformIntegerHyperparameter(\"max_features\", 2, 10, default_value=4)\ncs.add_hyperparameters([min_samples_split, max_features])\n\nsubsample = UniformFloatHyperparameter(\"subsample\", 0.5, 1, default_value=0.8)\ncs.add_hyperparameter(subsample)\n\nprint(\"default cross validation score: %.2f\" % (xgboost_from_cfg(cs.get_default_configuration())))\ncfg = cs.get_default_configuration()\nclf = GradientBoostingClassifier(**cfg, random_state=0).fit(X_train, y_train)\ndef_test_score = 1 - clf.score(X_test, y_test)\nprint(\"default test score: %.2f\" % def_test_score)\n\n# scenario object\nscenario = Scenario({\"run_obj\": \"quality\",\n                     \"runcount-limit\": 100,\n                     \"cs\": cs,\n                     \"deterministic\": \"true\",\n                     \"wallclock_limit\": 120\n                     })\n\n\nsmac = SMAC4HPO(scenario=scenario, rng=np.random.RandomState(0), tae_runner=xgboost_from_cfg)\n\n# the optimization process is called\nincumbent = smac.optimize()\n\n# a classifier is trained with the hyperparameters returned from the optimizer\nclf_incumbent = GradientBoostingClassifier(**incumbent, random_state=0).fit(X_train, y_train)\n\n# evaluated on test\ninc_value_1 = 1 - clf_incumbent.score(X_test, y_test)\nprint(\"Score on test set: %.2f\" % (inc_value_1))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}